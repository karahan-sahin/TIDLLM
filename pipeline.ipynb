{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9opm1INuyxIU"},"outputs":[],"source":["%pip install --upgrade --quiet langchain langchain-openai mediapipe\n","%pip install -r requirements.txt"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":14748,"status":"ok","timestamp":1715370875731,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"O0WNVwRK9isM"},"outputs":[],"source":["!cp -r /content/drive/MyDrive/TIDLLM/lib/ ."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715370875731,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"dIsN5tj43qjr","outputId":"c616df7f-f6c3-4093-b413-8b61930c7bd4"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/TIDLLM\n"]}],"source":["%cd \"/content/drive/MyDrive/TIDLLM\""]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":14422,"status":"ok","timestamp":1715370890151,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"dOpRdOwgwXgD"},"outputs":[],"source":["import os\n","import re\n","from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","import glob\n","import matplotlib.pyplot as plt\n","\n","import mediapipe as mp\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import tarfile"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1715370890151,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"k4SFZO-X9zr4"},"outputs":[],"source":["from lib.utils.pose import *"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715370890152,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"bO2jku3E7HM-"},"outputs":[],"source":["def get_pose_array(SAMPLE_POSE):\n","    \"\"\"Converts the pose data into a numpy array\n","    \"\"\"\n","\n","    POSE_RAW = pd.DataFrame(SAMPLE_POSE['pose'])\n","    RIGHT_HAND_RAW = pd.DataFrame(SAMPLE_POSE['right'])\n","    LEFT_HAND_RAW = pd.DataFrame(SAMPLE_POSE['left'])\n","\n","    POSE_DF = {}\n","\n","    for col in POSE_RAW.columns:\n","        POSE_DF[ 'POSE_' + col + '_X'] = POSE_RAW[col].apply(lambda x: x[0])\n","        POSE_DF[ 'POSE_' + col + '_Y'] = POSE_RAW[col].apply(lambda x: x[1])\n","\n","    for col in RIGHT_HAND_RAW.columns:\n","        POSE_DF[ 'RIGHT_' + col + '_X' ] = RIGHT_HAND_RAW[col].apply(lambda x: x[0])\n","        POSE_DF[ 'RIGHT_' + col + '_Y' ] = RIGHT_HAND_RAW[col].apply(lambda x: x[1])\n","\n","    for col in LEFT_HAND_RAW.columns:\n","        POSE_DF[ 'LEFT_' + col + '_X' ] = LEFT_HAND_RAW[col].apply(lambda x: x[0])\n","        POSE_DF[ 'LEFT_' + col + '_Y' ] = LEFT_HAND_RAW[col].apply(lambda x: x[1])\n","\n","    POSE_DF = pd.DataFrame(POSE_DF)\n","\n","    return POSE_DF\n","\n","\n","def get_matrices(POSE_DF):\n","    \"\"\"Converts the pose data into a numpy array of distance matrices\n","    \"\"\"\n","    x_cols = [col for col in POSE_DF.columns if col.endswith('_X')]\n","    y_cols = [col for col in POSE_DF.columns if col.endswith('_Y')]\n","\n","    frames = []\n","    for i in range(1, POSE_DF.shape[0]):\n","        x_row = POSE_DF[x_cols].iloc[i].to_numpy()\n","        y_row = POSE_DF[y_cols].iloc[i].to_numpy()\n","\n","        def get_difference_matrix(row):\n","            m, n = np.meshgrid(row, row)\n","            out = m-n\n","            return out\n","\n","        x_diff = get_difference_matrix(x_row)\n","        y_diff = get_difference_matrix(y_row)\n","\n","        frame = np.stack([x_diff, y_diff], axis=2)\n","        frames.append(frame)\n","\n","    frames = np.stack(frames, axis=0)\n","    return frames"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1715370890152,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"zrvyKJbF0wVW"},"outputs":[],"source":["def gloss2text(GLOSSES):\n","    \"\"\"Translate set of glosses to text\n","    \"\"\"\n","    return chat_bot.invoke(\n","        [\n","            HumanMessage(\n","                content=f\"\"\"\n","                    Kelimelerin girdisinden anlamlı türkçe cümle oluştur\n","\n","                    Girdi:{GLOSSES}\n","                    Cümle:\n","                \"\"\"\n","            )\n","        ]\n","    ).content"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1715370890152,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"g_Js4G3p-sme"},"outputs":[],"source":["class GlossClassifier(nn.Module):\n","    def __init__(self, input_size, hidden_dim, num_layers, out_size):\n","        super(GlossClassifier, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.out_size = out_size\n","\n","        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_dim, out_size)\n","        self.dropout = nn.Dropout(0.2)\n","\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n","        x = self.dropout(x)\n","        out, (ht, ct) = self.lstm(x, (h0, c0))\n","        return self.linear(ht[-1])\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1715370890152,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"vVvrc6YX1SUC"},"outputs":[],"source":["class Quantizer(nn.Module):\n","    \"\"\"\n","    Quantizer for Continuous Sign Language Recognition\n","\n","    Args:\n","        quantizer (nn.Module): base vq-vae based quantizer model\n","        num_frames (int): number of frames to consider\n","        stride (int): stride of the sliding window\n","\n","    ### A. Usage:\n","\n","    Define the quantizer model\n","\n","    ```python\n","    quantizer = Quantizer(quantizer, num_frames=25, stride=1)\n","    quantized, indices = quantizer.quantize(x)\n","    ```\n","\n","    If you want to transform the indices into one-hot, set `transform=True`\n","\n","    ```python\n","    quantized, indices = quantizer.quantize(x, transform=True)\n","    ```\n","\n","    Also process the video to get the pose estimation\n","\n","    ```python\n","    x = Quantizer.process_video(video_path)\n","    ```\n","    \"\"\"\n","\n","    def __init__(self,\n","                 base_model,\n","                 num_frames=25,\n","                 stride=1,\n","                 num_codebooks=1,\n","                 codebook_size=512):\n","        super(Quantizer, self).__init__()\n","\n","        self.quantizer = torch.load(\n","            base_model,\n","            map_location=torch.device('cpu' if not torch.cuda.is_available() else 'cuda:0')\n","        )\n","\n","        self.quantizer.eval()\n","\n","        self.num_frames = num_frames\n","        self.stride = stride\n","\n","        self.codebook_size = codebook_size\n","        self.num_codebooks = num_codebooks\n","\n","    @torch.no_grad()\n","    def encode(self, x):\n","        x_hat = self.quantizer.encoder(x)\n","        quantized, indices, _ = self.quantizer.vq_vae(x_hat)\n","        return quantized, indices\n","\n","    def quantize(self, x, transform=False):\n","        x = x.permute(0, 4, 1, 2, 3).contiguous()\n","        # NOTE: x.shape = (B, C, D, H, W)\n","        # Slice input channels\n","        x = x[:, :self.quantizer.encoder.input_size[1], :, :, :]\n","        quantized, indices = self.encode(x)\n","        if transform:\n","            indices = indices.view(-1, self.num_codebooks)\n","            # turn indices into one-hot\n","            indices = F.one_hot(indices, num_classes= self.num_codebooks * self.codebook_size).float()\n","\n","        return quantized, indices\n","\n","    def process_video(self, video_path: str):\n","\n","        print('** Get Pose Estimation **')\n","        pose, _ = get_pose_estimation(video_path)\n","        print('** Translate Pose Array **')\n","        pose_array = get_pose_array(pose)\n","        # Replace missing values with zeros\n","        pose_array = pose_array.replace(np.nan, 0)\n","        matrices = get_matrices(pose_array)\n","\n","        # Generate overlapping windows\n","        print('** Get windows **')\n","        windows = []\n","        for i in range(0, len(matrices) - self.num_frames, self.stride):\n","            window = matrices[i:i+self.num_frames]\n","            windows.append(window)\n","\n","        # Convert to tensor\n","        data = torch.tensor(windows).float()\n","\n","        return data\n","\n","    def process_pose(self, pose_array):\n","\n","        pose_array = pose_array.replace(np.nan, 0)\n","        matrices = get_matrices(pose_array)\n","\n","        # Generate overlapping windows\n","        windows = []\n","        for i in range(0, len(matrices) - self.num_frames, self.stride):\n","            window = matrices[i:i+self.num_frames]\n","            windows.append(window)\n","\n","        # Convert to tensor\n","        data = torch.tensor(windows).float()\n","\n","        return data"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":555,"status":"ok","timestamp":1715370890697,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"QqED3PSb0r30"},"outputs":[],"source":["chat_bot = ChatOpenAI(\n","    model=\"gpt-3.5-turbo-1106\",\n","    organization='###',\n","    temperature=0.0,\n","    api_key='###'\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715370890697,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"7nc_k1T25M2K"},"outputs":[],"source":["QUANTIZER_PATH = 'quantizer/encoder.pt'\n","GLOSS_MODEL_PATH = '/content/drive/MyDrive/TIDLLM/dataset/bsign22k/pose_arrayentire_model.pt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ugCTYgIEttO"},"outputs":[],"source":["!pip install vector_quantize_pytorch"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":4212,"status":"ok","timestamp":1715371708467,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"c2-viezX3kV-"},"outputs":[],"source":["quantizer = Quantizer(\n","    base_model=QUANTIZER_PATH,\n","    num_frames=25,\n","    stride=5,\n","    num_codebooks=5,\n","    codebook_size=128,\n",")"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715371708467,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"qjQGEufv5DyG"},"outputs":[],"source":["gloss_model = torch.load(GLOSS_MODEL_PATH)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715371708468,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"6Lk3unJ6EV0c","outputId":"644d4f25-93ee-4608-86d1-31a8e297ab3d"},"outputs":[{"data":{"text/plain":["GlossClassifier(\n","  (lstm): LSTM(768, 256, num_layers=2, batch_first=True)\n","  (linear): Linear(in_features=256, out_features=745, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["quantizer.to('cuda:0')\n","gloss_model.to('cuda:0')"]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":409,"status":"ok","timestamp":1715372661744,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"y_d7f3Iw4ZDc"},"outputs":[],"source":["import json\n","\n","class SignTranslation():\n","\n","    def __init__(self,\n","                 quantizer,\n","                 gloss_model):\n","\n","        self.quantizer = quantizer\n","        self.gloss_model = gloss_model\n","\n","        self.ID2GLOSS = json.loads(open('/content/drive/MyDrive/TIDLLM/dataset/bsign22k/pose_arraygloss_id_pairs.json', 'r').read())\n","\n","\n","    def translate(self, video, frames = 10, stride = 1):\n","\n","        print('** Processing video **')\n","        poses = self.quantizer.process_video(video)\n","\n","        print('** Quantize video **')\n","        quantization, _ = quantizer.quantize(poses.to('cuda:0'))\n","\n","        batch = []\n","        matrix_shape = quantization.shape[0]\n","        for i in range(0, matrix_shape - frames, stride):\n","            window = quantization[None,i:i+frames,:]\n","            batch.append(window)\n","\n","        batched = torch.cat(batch, axis=0)\n","\n","        print('** Predicting Glossess **')\n","        outputs = self.gloss_model(batched.to('cuda:0'))\n","\n","        gloss_list = []\n","        for cls in outputs.argmax(0):\n","            gloss_list.append(self.ID2GLOSS[str(cls.item()+1)])\n","\n","        gloss_sent = ''\n","        current_gloss = None\n","        for gloss in gloss_list:\n","            if current_gloss != gloss:\n","                gloss_sent += f'{gloss} '\n","\n","        return gloss2text(gloss_sent)"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715372662080,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"27SpI_9oKrft"},"outputs":[],"source":["translator = SignTranslation(\n","    quantizer=quantizer,\n","    gloss_model=gloss_model,\n",")"]},{"cell_type":"code","execution_count":74,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715372662080,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"UyfxWtOs4cHG"},"outputs":[],"source":["#gloss2text('BEN BUGÜN YEMEK PAZAR BEN GİTMEK ELMA GRUP PORTAKAL GRUP YEMEK ÇEŞİT GÖRE GRUP GRUP BEN BİR SEÇMEK SEÇMEK')"]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28424,"status":"ok","timestamp":1715372953641,"user":{"displayName":"aiprojects-fgr inzva","userId":"02121075453386086779"},"user_tz":-180},"id":"05yRb_GN8t1g","outputId":"830e445d-101f-4dcf-fc1d-0b4bfc562b6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["** Processing video **\n","** Get Pose Estimation **\n","** Translate Pose Array **\n","** Get windows **\n","** Quantize video **\n","torch.Size([9, 10, 768])\n","** Predicting Glossess **\n","Translation: Acil servis, acılarınızı hafifletmek için burada.\n","Took seconds: 28.154761472000246\n"]}],"source":["import time\n","\n","t1 = time.perf_counter()\n","\n","translation = translator.translate('/content/drive/MyDrive/TIDLLM/dataset/sample.mp4', frames=10, stride=3)\n","\n","t2 = time.perf_counter()\n","\n","print(f'Translation: {translation}')\n","print(f'Took seconds: {t2-t1}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KfyjkHI5KyvC"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMOF42hKSEHr4vOa7liVcsZ","gpuType":"T4","mount_file_id":"1g61cZ0QrVfIULz5IN0eGaALIuhsjB2Vq","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
