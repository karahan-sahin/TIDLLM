{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install opencv-python\n",
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lib.config import *\n",
    "\n",
    "import vector_quantize_pytorch as vq\n",
    "from lib.encoder.vqvae import VQVAE_POSE\n",
    "from lib.utils.dataset import get_dataset\n",
    "from lib.train.autoencoder import AutoTrainer\n",
    "from lib.data.dataset import PoseDistanceDataset, PoseDataset\n",
    "from lib.encoder.cnn import CNN3dEncoder, CNN3dDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MODEL_ENCODER = CNN3dEncoder(\n",
    "    model_name=GLOBAL_CONFIG.MODEL_ENCODER_NAME,\n",
    "    conv_layers=GLOBAL_CONFIG.MODEL_ENCODER_CONVOLUTIONAL_LAYERS,\n",
    "    linear_layers=GLOBAL_CONFIG.MODEL_ENCODER_LINEAR_LAYERS,\n",
    "    out_channels=GLOBAL_CONFIG.MODEL_ENCODER_OUT_CHANNEL,\n",
    "    input_size=GLOBAL_CONFIG.INPUT_DIM,\n",
    "    output_size=GLOBAL_CONFIG.MODEL_VQ_EMBED_DIM,\n",
    "    channel_size=GLOBAL_CONFIG.INPUT_CHANNELS,\n",
    "    depth_size=GLOBAL_CONFIG.FRAME_WINDOW,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "MODEL_DECODER = CNN3dDecoder(\n",
    "    model_name=GLOBAL_CONFIG.MODEL_DECODER_NAME,\n",
    "    linear_layers=GLOBAL_CONFIG.MODEL_DECODER_LINEAR_LAYERS,\n",
    "    conv_transpose_layers=GLOBAL_CONFIG.MODEL_DECODER_CONVOLUTIONAL_LAYERS,\n",
    "    in_channels=GLOBAL_CONFIG.MODEL_ENCODER_OUT_CHANNEL,\n",
    "    linear_input=GLOBAL_CONFIG.MODEL_VQ_EMBED_DIM,\n",
    "    input_size=MODEL_ENCODER.output_size, # This is for reshaping into encoder before linear layers\n",
    "    output_size=(GLOBAL_CONFIG.INPUT_CHANNELS, GLOBAL_CONFIG.FRAME_WINDOW, GLOBAL_CONFIG.INPUT_DIM[3], GLOBAL_CONFIG.INPUT_DIM[4]),\n",
    "    log=False\n",
    ")\n",
    "\n",
    "VQVAE = vq.ResidualVQ(\n",
    "    dim=GLOBAL_CONFIG.MODEL_VQ_EMBED_DIM,\n",
    "    codebook_size=GLOBAL_CONFIG.MODEL_VQ_VOCAB,\n",
    "    num_quantizers=GLOBAL_CONFIG.MODEL_VQ_CODEBOOK,\n",
    "    codebook_dim=GLOBAL_CONFIG.MODEL_VQ_EMBED_DIM,\n",
    ")\n",
    "\n",
    "MODEL_VQVAE = VQVAE_POSE(\n",
    "    encoder=MODEL_ENCODER,\n",
    "    decoder=MODEL_DECODER,\n",
    "    vq_vae=VQVAE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = get_dataset(\n",
    "    DATASET_PATH='dataset/adjacency/',\n",
    "    DATASET_EXTENSION='.npy',\n",
    "    DATASET_ENCODING='utf-8',\n",
    "    DATA_DISTRIBUTION='80-20',\n",
    "    DATASET_CONFIG={\n",
    "        'window': GLOBAL_CONFIG.FRAME_WINDOW,\n",
    "        'depth': GLOBAL_CONFIG.INPUT_CHANNELS\n",
    "    },\n",
    "    RANDOM_STATE=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_PATH = 'experiments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "EXPERIMENT_NAME = str(datetime.now())\n",
    "os.mkdir(f'{EXPERIMENT_PATH}/{EXPERIMENT_NAME}')\n",
    "os.mkdir(f'{EXPERIMENT_PATH}/{EXPERIMENT_NAME}/model')\n",
    "os.mkdir(f'{EXPERIMENT_PATH}/{EXPERIMENT_NAME}/logs')\n",
    "EXPERIMENT_PATH = EXPERIMENT_PATH + '/' + EXPERIMENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AutoTrainer(\n",
    "    model=MODEL_VQVAE,\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=eval_dataset,\n",
    "    batch_size=GLOBAL_CONFIG.BATCH_SIZE,\n",
    "    epochs=GLOBAL_CONFIG.NUM_EPOCHS,\n",
    "    learning_rate=GLOBAL_CONFIG.LEARNING_RATE,\n",
    "    step_size=GLOBAL_CONFIG.STEP_SIZE,\n",
    "    gamma=GLOBAL_CONFIG.GAMMA,\n",
    "    device='cpu',\n",
    "    start_epoch=0,\n",
    "    num_codebooks=GLOBAL_CONFIG.MODEL_VQ_CODEBOOK,\n",
    "    model_path=f'{EXPERIMENT_PATH}/model/model-0.pt',\n",
    "    log_dir=f'{EXPERIMENT_PATH}/logs/logs.json'\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer model out\n",
    "from lib.utils.infer import *\n",
    "\n",
    "df = get_quantization(MODEL_VQVAE, eval_dataset)\n",
    "\n",
    "dump_quantization(\n",
    "    df, \n",
    "    num_quantizers=GLOBAL_CONFIG.MODEL_VQ_CODEBOOK, \n",
    "    video_path='dataset/corpus', \n",
    "    quantization_path='analyze/quantization'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pose Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = 'dataset/corpus/ABARTMAK_0.mp4'\n",
    "SAMPLE_POSE = get_pose_estimation(SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_pose_array(SAMPLE_POSE):\n",
    "    \"\"\"Converts the pose data into a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    POSE_RAW = pd.DataFrame(SAMPLE_POSE['pose'])\n",
    "    RIGHT_HAND_RAW = pd.DataFrame(SAMPLE_POSE['right'])\n",
    "    LEFT_HAND_RAW = pd.DataFrame(SAMPLE_POSE['left'])\n",
    "\n",
    "    POSE_DF = {}\n",
    "\n",
    "    for col in POSE_RAW.columns:\n",
    "        POSE_DF[ 'POSE_' + col + '_X'] = POSE_RAW[col].apply(lambda x: x[0])\n",
    "        POSE_DF[ 'POSE_' + col + '_Y'] = POSE_RAW[col].apply(lambda x: x[1])\n",
    "        POSE_DF[ 'POSE_' + col + '_Z'] = POSE_RAW[col].apply(lambda x: x[2])\n",
    "        # POSE_DF[col + '_viz'] = POSE_RAW[col].apply(lambda x: x[3])\n",
    "\n",
    "    for col in RIGHT_HAND_RAW.columns:\n",
    "        POSE_DF[ 'RIGHT_' + col + '_X' ] = RIGHT_HAND_RAW[col].apply(lambda x: x[0])\n",
    "        POSE_DF[ 'RIGHT_' + col + '_Y' ] = RIGHT_HAND_RAW[col].apply(lambda x: x[1])\n",
    "        POSE_DF[ 'RIGHT_' + col + '_Z' ] = RIGHT_HAND_RAW[col].apply(lambda x: x[2])\n",
    "        # POSE_DF['RIGHT_' + col + '_viz'] = RIGHT_HAND_RAW[col].apply(lambda x: x[3])\n",
    "\n",
    "    for col in LEFT_HAND_RAW.columns:\n",
    "        POSE_DF[ 'LEFT_' + col + '_X' ] = LEFT_HAND_RAW[col].apply(lambda x: x[0])\n",
    "        POSE_DF[ 'LEFT_' + col + '_Y' ] = LEFT_HAND_RAW[col].apply(lambda x: x[1])\n",
    "        POSE_DF[ 'LEFT_' + col + '_Z' ] = LEFT_HAND_RAW[col].apply(lambda x: x[2])\n",
    "        # POSE_DF['LEFT_' + col + '_viz'] = LEFT_HAND_RAW[col].apply(lambda x: x[3])\n",
    "\n",
    "    POSE_DF = pd.DataFrame(POSE_DF)\n",
    "\n",
    "    return POSE_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_DF = get_pose_array(SAMPLE_POSE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_DF.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_DF = POSE_DF.replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(POSE_DF):\n",
    "    \"\"\"Converts the pose data into a numpy array of distance matrices\n",
    "    \"\"\"\n",
    "    x_cols = [col for col in POSE_DF.columns if col.endswith('_X')]\n",
    "    y_cols = [col for col in POSE_DF.columns if col.endswith('_Y')]\n",
    "    z_cols = [col for col in POSE_DF.columns if col.endswith('_Z')]\n",
    "\n",
    "    frames = []\n",
    "    for i in range(1, POSE_DF.shape[0]):\n",
    "        x_row = POSE_DF[x_cols].iloc[i].to_numpy()\n",
    "        y_row = POSE_DF[y_cols].iloc[i].to_numpy()\n",
    "        z_row = POSE_DF[z_cols].iloc[i].to_numpy()\n",
    "\n",
    "        def get_difference_matrix(row):\n",
    "            m, n = np.meshgrid(row, row)\n",
    "            out = m-n\n",
    "            return out\n",
    "\n",
    "        x_diff = get_difference_matrix(x_row)\n",
    "        y_diff = get_difference_matrix(y_row)\n",
    "        z_diff = get_difference_matrix(z_row)\n",
    "\n",
    "        frame = np.stack([x_diff, y_diff, z_diff], axis=2)\n",
    "        frames.append(frame)\n",
    "\n",
    "    frames = np.stack(frames, axis=0)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = 'dataset/adjacency'\n",
    "POSE_PATH = 'dataset/pose'\n",
    "\n",
    "for file in tqdm(glob.glob('dataset/pose/*.npy')):\n",
    "    if os.path.exists(os.path.join(OUT_PATH, os.path.basename(file).replace('.mp4', '.npy'))):\n",
    "        # print('Skipping', file)\n",
    "        continue\n",
    "    with open(file, 'rb') as f:\n",
    "        array = np.load(f, allow_pickle=True)\n",
    "        # replace nan with 0 \n",
    "        array = np.nan_to_num(array)\n",
    "    pose_df = pd.DataFrame(array, columns=POSE_DF.columns)\n",
    "    pose_df = pose_df.replace(np.nan,0)\n",
    "    MATRICES = get_matrices(pose_df)\n",
    "    # print(MATRICES.shape)\n",
    "    np.save(os.path.join(OUT_PATH, os.path.basename(file).replace('.mp4', '.npy')), MATRICES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(x_diff, columns=[col for col in POSE_DF.columns if col.endswith('_X')], index=[col for col in POSE_DF.columns if col.endswith('_X')]).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    ARRAY_PATH = 'dataset/pose/'\n",
    "    for datapath in tqdm(glob.glob('dataset/corpus/*.mp4')):\n",
    "        print(datapath)\n",
    "        pose, _ = get_pose_estimation(datapath)\n",
    "        pose_array = get_pose_array(pose)\n",
    "        print(pose_array.shape, datapath)\n",
    "        dname = datapath.split('/')[-1].replace('.mp4', '.npy')\n",
    "        with open(ARRAY_PATH+'/'+dname, 'wb') as f:\n",
    "            np.save(f, pose_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Graph Autoencoder Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'dataset/pose/'\n",
    "data = glob.glob(DATA_PATH + '*.npy')\n",
    "X_train, X_val = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PoseDataset(X_train)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=GLOBAL_CONFIG.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = PoseDataset(X_val)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=GLOBAL_CONFIG.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MODEL_ENCODER = FFNEncoder(\n",
    "    input_dim=GLOBAL_CONFIG.MODEL_ENCODER_INPUT_DIM,\n",
    "    hidden_dim=GLOBAL_CONFIG.MODEL_ENCODER_HIDDEN_DIM,\n",
    "    output_dim=GLOBAL_CONFIG.MODEL_ENCODER_OUTPUT_DIM,\n",
    ")\n",
    "\n",
    "MODEL_DECODER = FFNDecoder(\n",
    "    input_dim=GLOBAL_CONFIG.MODEL_DECODER_INPUT_DIM,\n",
    "    hidden_dim=GLOBAL_CONFIG.MODEL_DECODER_HIDDEN_DIM,\n",
    "    output_dim=GLOBAL_CONFIG.MODEL_ENCODER_INPUT_DIM,\n",
    ")\n",
    "\n",
    "MODEL_QUANT = ResidualVQ(\n",
    "    dim = GLOBAL_CONFIG.MODEL_VQ_EMBED_DIM,\n",
    "    stochastic_sample_codes=True,\n",
    "    num_quantizers=1,      # specify number of quantizers\n",
    "    codebook_size=GLOBAL_CONFIG.MODEL_VQ_NUM_EMBS,    # codebook size           \n",
    "    kmeans_init=True,   # set to True\n",
    "    kmeans_iters=100     # number of kmeans iterations to calculate the centroids for the codebook on init\n",
    ")\n",
    "\n",
    "MODEL_VQVAE = VQVAE(\n",
    "    encoder=MODEL_ENCODER,\n",
    "    decoder=MODEL_DECODER,\n",
    "    vq=MODEL_QUANT,\n",
    ")\n",
    "\n",
    "trainer = AutoencoderTrainer(\n",
    "    model=MODEL_VQVAE,\n",
    "    learning_rate=GLOBAL_CONFIG.LEARNING_RATE,\n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader,\n",
    "    num_epochs=GLOBAL_CONFIG.NUM_EPOCHS,\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "MODEL_VQVAE.eval()\n",
    "\n",
    "dfs = []\n",
    "for train_sample in tqdm(train_dataloader):\n",
    "    with torch.no_grad():\n",
    "        quantized, indices, commitment_loss = MODEL_VQVAE(train_sample['array'].float())\n",
    "        dfs.append(pd.DataFrame({\n",
    "            'videos': train_sample['token'],\n",
    "            'labels': indices.detach().cpu().numpy().reshape(-1),\n",
    "            'frame': train_sample['frame'].detach().cpu().numpy().reshape(-1)\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "for rec in tqdm(df[df['labels'] == 375].to_dict(orient='records')[:100]):\n",
    "    # save frame video to disk\n",
    "    video = rec['videos'].split('.')[0]\n",
    "    video_path = f\"dataset/corpus/{video}.mp4\"\n",
    "    frame_idx = rec['frame']\n",
    "    label = rec['labels']\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not os.path.exists(f'analyze/quantization/{label}'): os.mkdir(f'analyze/quantization/{label}')\n",
    "\n",
    "    for i in range(frame_idx):\n",
    "        ret, frame = cap.read()\n",
    "        if i == frame_idx-1:\n",
    "            cv2.imwrite(f'analyze/quantization/{label}/{video}_{frame_idx}.jpg', frame)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. 3D-CNN Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'dataset/adjacency/'\n",
    "data = glob.glob(DATA_PATH + '*.npy')[:100]\n",
    "X_train, X_val = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PoseDistanceDataset(X_train)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=GLOBAL_CONFIG.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=PoseDistanceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = PoseDistanceDataset(X_val)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=GLOBAL_CONFIG.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=PoseDistanceDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MODEL_ENCODER = CNNEncoder(\n",
    "    input_channels=3,\n",
    ")\n",
    "\n",
    "MODEL_DECODER = CNNDecoder(\n",
    "    output_channels=3,\n",
    ")\n",
    "\n",
    "MODEL_QUANT = ResidualVQ(\n",
    "    dim = GLOBAL_CONFIG.MODEL_VQ_EMBED_DIM,\n",
    "    stochastic_sample_codes=True,\n",
    "    num_quantizers=1,      # specify number of quantizers\n",
    "    codebook_size=GLOBAL_CONFIG.MODEL_VQ_NUM_EMBS,    # codebook size           \n",
    "    kmeans_init=True,   # set to True\n",
    "    kmeans_iters=10     # number of kmeans iterations to calculate the centroids for the codebook on init\n",
    ")\n",
    "\n",
    "MODEL_VQVAE = VQVAE(\n",
    "    encoder=MODEL_ENCODER,\n",
    "    decoder=MODEL_DECODER,\n",
    "    vq=MODEL_QUANT,\n",
    ")\n",
    "\n",
    "trainer = AutoencoderTrainer(\n",
    "    model=MODEL_VQVAE,\n",
    "    learning_rate=GLOBAL_CONFIG.LEARNING_RATE,\n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader,\n",
    "    num_epochs=GLOBAL_CONFIG.NUM_EPOCHS,\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "MODEL_VQVAE.eval()\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=10, \n",
    "    shuffle=True,\n",
    "    collate_fn=train_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "dfs = []\n",
    "for train_sample in tqdm(train_dataloader):\n",
    "    with torch.no_grad():\n",
    "        quantized, indices, commitment_loss = MODEL_VQVAE(train_sample['array'].float())\n",
    "\n",
    "        quant = {\n",
    "            'videos': train_sample['tokens'],\n",
    "            'start_idx': train_sample['start_idx'],\n",
    "            'end_idx': train_sample['end_idx']\n",
    "        }\n",
    "        \n",
    "\n",
    "        for index in range(indices.shape[1]):\n",
    "            quant[f'Code_{index}'] = indices[:, index].cpu().numpy()\n",
    "\n",
    "\n",
    "        dfs.append(pd.DataFrame(quant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "LOG_IDX=45\n",
    "with open(f'analyze/quantization/experimental_logs/logs-{LOG_IDX}.json', 'r') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = corpus['train']['commit-loss']\n",
    "import numpy as np\n",
    "for i in range(len(cls)):\n",
    "    if cls[str(i)]:\n",
    "        print(i,np.sum(cls[str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = corpus['validation']\n",
    "df = {}\n",
    "for keys in ['vocab', 'start_idx', 'end_idx', 'quantization']:\n",
    "    print(keys)\n",
    "    print(val[keys][f'{LOG_IDX}'])\n",
    "    if keys == 'quantization':\n",
    "        for code in val[keys][f'{LOG_IDX}']:\n",
    "            df[code] = val[keys][f'{LOG_IDX}'][code]\n",
    "    else:\n",
    "        df[keys] = val[keys][f'{LOG_IDX}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(df)\n",
    "df.start_idx = df.start_idx.astype(int)\n",
    "df.end_idx = df.end_idx.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Code_0.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Code_1.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "CODEBOOK = 'Code_1'\n",
    "CODE_ID = 606           \n",
    "\n",
    "for rec in tqdm(df[df[CODEBOOK] == CODE_ID].to_dict(orient='records')):\n",
    "    # save frame video to disk\n",
    "    video = rec['vocab']\n",
    "    video_path = f\"dataset/corpus/{video}.mp4\"\n",
    "    start_idx = rec['start_idx']\n",
    "    end_idx = rec['end_idx']\n",
    "    label = str(rec['Code_1']) + '-' + str(rec['Code_0'])\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    import os\n",
    "    if not os.path.exists(f'analyze/quantization/{label}'):\n",
    "        os.mkdir(f'analyze/quantization/{label}')\n",
    "\n",
    "    FRAMES = []\n",
    "    for i in range(end_idx+1):\n",
    "        ret, frame = cap.read()\n",
    "        if i >= start_idx and i < end_idx:\n",
    "            FRAMES.append(frame)\n",
    "\n",
    "    # write frames to video\n",
    "    out = cv2.VideoWriter(f'analyze/quantization/{label}/{video}_{start_idx}_{end_idx}.avi', cv2.VideoWriter_fourcc(*'DIVX'), 15, (frame.shape[1], frame.shape[0]))\n",
    "    for frame in FRAMES:\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()      \n",
    "\n",
    "    videoClip = VideoFileClip(f\"analyze/quantization/{label}/{video}_{start_idx}_{end_idx}.avi\")\n",
    "    videoClip.write_gif(f\"analyze/quantization/{label}/{video}_{start_idx}_{end_idx}.gif\")\n",
    "\n",
    "    os.remove(f\"analyze/quantization/{label}/{video}_{start_idx}_{end_idx}.avi\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
